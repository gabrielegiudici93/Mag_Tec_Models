#!/usr/bin/env python3
"""
Single-point Stretch Validation Pipeline
----------------------------------------

This script loads the single-point data collected across multiple stretch levels,
trains simple machine-learning models, and reports the key performance metrics:

    • Force mapping accuracy (RMSE) and precision (STD of residuals) per stretch level
    • Offset classification accuracy (which of the 5 offsets is pressed)
    • Stretch classification accuracy (0%, 5%, 10%, ...)
    • An observed force resolution estimate (minimum delta in recorded forces)

The script expects the data generated by `franka_skin_test_single_point.py`, which
stores per-press summaries inside each HDF5 file (`press_summaries/...` datasets).
Each acquisition run is saved under a dedicated subdirectory:

    data/
        run_20251110_1820/
            stretch_000pct/
                *.h5
            stretch_010pct/
                *.h5
            stretch_020pct/
                *.h5
        run_custom_label/
            stretch_000pct/ ...

Usage:
    python3 evaluate_single_point_stretch.py [--data-root PATH] [--run RUN_DIR] [--report PATH]
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import h5py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
from sklearn.model_selection import train_test_split

# ---------------------------------------------------------------------------
# Configuration & paths
# ---------------------------------------------------------------------------

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from franka_controller.config import DATA_DIR, LOGS_DIR  # noqa: E402

DEFAULT_DATA_ROOT = Path(DATA_DIR)
REPORTS_DIR = Path(LOGS_DIR) / "reports"
REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# KPM thresholds
ACCURACY_RMSE_THRESHOLD = 0.10  # High accuracy → RMSE < 0.1 N
PRECISION_STD_THRESHOLD = 0.05  # High precision → std dev < 0.05 N
SENSITIVITY_TARGET = 0.05       # Desired force resolution (N)

# Central neighbourhood sensor indices (0-based: corresponds to sensors 2,4,8,12,14 in 1-based)
CENTRAL_SENSOR_INDICES = [1, 3, 7, 11, 13]

# ---------------------------------------------------------------------------
# Data loading utilities
# ---------------------------------------------------------------------------


def load_press_summaries(h5_path: Path) -> pd.DataFrame:
    """Load per-press summaries from a single HDF5 file."""
    records: List[Dict] = []

    with h5py.File(h5_path, "r") as hf:
        if "press_summaries/sensors" not in hf:
            return pd.DataFrame()

        sensors = hf["press_summaries/sensors"][:]           # (n_press, 15, 3)
        forces = hf["press_summaries/forces"][:]             # (n_press, 6)
        metadata_raw = hf["press_summaries/metadata"][:]     # (n_press,)

        for sensor_snapshot, force_snapshot, meta_raw in zip(sensors, forces, metadata_raw):
            try:
                meta = json.loads(meta_raw.decode("utf-8"))
            except Exception:
                meta = {}

            subset_snapshot = sensor_snapshot[CENTRAL_SENSOR_INDICES, :]

            record = {
                "source_file": str(h5_path),
                "file": h5_path.name,
                "stretch_label": meta.get("stretch_label", h5_path.parent.name),
                "stretch_value": meta.get("stretch_level", np.nan),
                "offset_key": meta.get("offset_key", "unknown"),
                "press_id": meta.get("press_id", ""),
                "press_index": meta.get("press_index", -1),
                "press_depth_m": meta.get("press_depth_m", np.nan),
                "sensor_vector": sensor_snapshot.reshape(-1),  # Flatten 15×3 → 45 features
                "sensor_vector_subset": subset_snapshot.reshape(-1),  # Flatten 5×3 → 15 features
                "force_vector": force_snapshot.astype(float),
                "fz": float(force_snapshot[2]),
            }
            records.append(record)

    return pd.DataFrame(records)


def load_dataset(data_root: Path) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """Load all press summaries grouped by stretch label."""
    all_rows: List[pd.DataFrame] = []
    grouped: Dict[str, pd.DataFrame] = {}

    stretch_dirs = [d for d in sorted(data_root.glob("stretch_*")) if d.is_dir()]

    if stretch_dirs:
        for stretch_dir in stretch_dirs:
            h5_files = sorted(stretch_dir.glob("*.h5"))
            if not h5_files:
                print(f"⚠️  No HDF5 files found in {stretch_dir}")
                continue
            latest_file = h5_files[-1]
            stretch_df = load_press_summaries(latest_file)
            if stretch_df.empty:
                print(f"⚠️  Press summaries missing in {latest_file}")
                continue

            grouped[stretch_dir.name] = stretch_df
            all_rows.append(stretch_df)
    else:
        h5_files = sorted(data_root.glob("*.h5"))
        if not h5_files:
            raise RuntimeError(f"No HDF5 files found in {data_root}")

        for h5_file in h5_files:
            stretch_df = load_press_summaries(h5_file)
            if stretch_df.empty:
                print(f"⚠️  Press summaries missing in {h5_file}")
                continue

            if "stretch_label" in stretch_df.columns and not stretch_df["stretch_label"].isnull().all():
                label = stretch_df["stretch_label"].iloc[0]
            else:
                label = h5_file.stem

            grouped.setdefault(label, []).append(stretch_df)
            all_rows.append(stretch_df)

        for label, frames in list(grouped.items()):
            grouped[label] = pd.concat(frames, ignore_index=True)

    if not all_rows:
        raise RuntimeError(f"No press summaries found under {data_root}")

    combined_df = pd.concat(all_rows, ignore_index=True)
    return combined_df, grouped


# ---------------------------------------------------------------------------
# Model training helpers
# ---------------------------------------------------------------------------


def train_force_regressor(X: np.ndarray, y: np.ndarray, label: str, train_ratio: float = 0.7):
    if len(X) < 5:
        raise ValueError(f"Not enough samples to train force regressor for {label}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, train_size=train_ratio, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))
    residuals = y_test - y_pred
    std_dev = float(np.std(residuals))

    unique_forces = np.unique(np.round(y, decimals=3))
    if len(unique_forces) > 1:
        deltas = np.diff(unique_forces)
        force_resolution = float(np.min(np.abs(deltas[np.abs(deltas) > 0])))
    else:
        force_resolution = float("nan")

    metrics = {
        "stretch_label": label,
        "samples": int(len(y)),
        "rmse": rmse,
        "std_dev": std_dev,
        "force_resolution_est": force_resolution,
        "kpm1_pass": force_resolution <= SENSITIVITY_TARGET if not np.isnan(force_resolution) else None,
        "kpm2_pass": (rmse < ACCURACY_RMSE_THRESHOLD) and (std_dev < PRECISION_STD_THRESHOLD),
    }

    return metrics, model


def train_offset_classifier(X: np.ndarray, y: np.ndarray, label: str, train_ratio: float = 0.7):
    if len(np.unique(y)) < 2:
        raise ValueError(f"Not enough class diversity to train offset classifier for {label}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, train_size=train_ratio, random_state=42, stratify=y
    )

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    report = classification_report(y_test, y_pred, zero_division=0)

    metrics = {
        "stretch_label": label,
        "samples": int(len(y)),
        "accuracy": accuracy,
        "report": report,
    }

    return metrics, model


def train_stretch_classifier(X: np.ndarray, y: np.ndarray, train_ratio: float = 0.7):
    if len(np.unique(y)) < 2:
        raise ValueError("Not enough stretch classes to train classifier.")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, train_size=train_ratio, random_state=42, stratify=y
    )

    model = RandomForestClassifier(
        n_estimators=250,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = float(accuracy_score(y_test, y_pred))
    report = classification_report(y_test, y_pred, zero_division=0)

    metrics = {
        "samples": int(len(y)),
        "accuracy": accuracy,
        "report": report,
    }

    return metrics, model


def evaluate_combined_pipeline(
    X: np.ndarray,
    offsets: np.ndarray,
    stretches: np.ndarray,
    fz: np.ndarray,
    train_ratio: float = 0.7,
):
    indices = np.arange(len(X))
    train_idx, test_idx = train_test_split(
        indices,
        train_size=train_ratio,
        random_state=42,
        stratify=stretches,
    )

    X_train, X_test = X[train_idx], X[test_idx]
    y_fz_train, y_fz_test = fz[train_idx], fz[test_idx]
    y_offset_train, y_offset_test = offsets[train_idx], offsets[test_idx]
    y_stretch_train, y_stretch_test = stretches[train_idx], stretches[test_idx]

    # Combined force regressor
    force_model = RandomForestRegressor(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    force_model.fit(X_train, y_fz_train)
    y_fz_pred = force_model.predict(X_test)
    rmse = float(np.sqrt(mean_squared_error(y_fz_test, y_fz_pred)))
    residuals = y_fz_test - y_fz_pred
    std_dev = float(np.std(residuals))

    unique_forces = np.unique(np.round(fz, decimals=3))
    if len(unique_forces) > 1:
        deltas = np.diff(unique_forces)
        force_resolution = float(np.min(np.abs(deltas[np.abs(deltas) > 0])))
    else:
        force_resolution = float("nan")

    combined_force_metrics = {
        "stretch_label": "combined",
        "samples": int(len(fz)),
        "rmse": rmse,
        "std_dev": std_dev,
        "force_resolution_est": force_resolution,
        "kpm1_pass": force_resolution <= SENSITIVITY_TARGET if not np.isnan(force_resolution) else None,
        "kpm2_pass": (rmse < ACCURACY_RMSE_THRESHOLD) and (std_dev < PRECISION_STD_THRESHOLD),
    }

    # Stretch classifier
    stretch_model = RandomForestClassifier(
        n_estimators=250,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    stretch_model.fit(X_train, y_stretch_train)
    stretch_pred_test = stretch_model.predict(X_test)
    stretch_accuracy = float(accuracy_score(y_stretch_test, stretch_pred_test))
    stretch_report = classification_report(y_stretch_test, stretch_pred_test, zero_division=0)
    combined_stretch_metrics = {
        "samples": int(len(y_stretch_test)),
        "accuracy": stretch_accuracy,
        "report": stretch_report,
    }

    # Combined offset classifier (pooled)
    combined_offset_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        n_jobs=-1,
    )
    combined_offset_model.fit(X_train, y_offset_train)
    combined_offset_pred = combined_offset_model.predict(X_test)
    combined_offset_accuracy = float(accuracy_score(y_offset_test, combined_offset_pred))
    combined_offset_report = classification_report(y_offset_test, combined_offset_pred, zero_division=0)
    combined_offset_metrics = {
        "samples": int(len(y_offset_test)),
        "accuracy": combined_offset_accuracy,
        "report": combined_offset_report,
    }

    # Per-stretch offset classifiers trained on training split
    unique_stretches = np.unique(stretches)
    per_stretch_models = {}
    for label in unique_stretches:
        mask_train = y_stretch_train == label
        if np.sum(mask_train) >= 2 and len(np.unique(y_offset_train[mask_train])) >= 2:
            clf = RandomForestClassifier(
                n_estimators=200,
                max_depth=None,
                random_state=42,
                n_jobs=-1,
            )
            clf.fit(X_train[mask_train], y_offset_train[mask_train])
            per_stretch_models[label] = clf
        else:
            per_stretch_models[label] = None

    # Gated offset prediction using stretch classifier
    gated_preds = []
    gated_true = []
    for x, true_offset in zip(X_test, y_offset_test):
        x_reshaped = x.reshape(1, -1)
        predicted_stretch = stretch_model.predict(x_reshaped)[0]
        clf = per_stretch_models.get(predicted_stretch)
        if clf is not None:
            pred_offset = clf.predict(x_reshaped)[0]
        else:
            pred_offset = combined_offset_model.predict(x_reshaped)[0]
        gated_preds.append(pred_offset)
        gated_true.append(true_offset)

    gated_accuracy = float(accuracy_score(gated_true, gated_preds))
    gated_report = classification_report(gated_true, gated_preds, zero_division=0)
    gated_offset_metrics = {
        "samples": int(len(gated_true)),
        "accuracy": gated_accuracy,
        "report": gated_report,
    }

    return {
        "combined_force_metrics": combined_force_metrics,
        "combined_stretch_metrics": combined_stretch_metrics,
        "combined_offset_metrics": combined_offset_metrics,
        "gated_offset_metrics": gated_offset_metrics,
    }

# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def parse_args():
    parser = argparse.ArgumentParser(description="Evaluate single-point stretch dataset.")
    parser.add_argument(
        "--data-root",
        type=Path,
        default=DEFAULT_DATA_ROOT,
        help="Root directory containing stretch_* subdirectories (default: config.DATA_DIR)",
    )
    parser.add_argument(
        "--run",
        type=str,
        default=None,
        help="Name of the acquisition run to evaluate (e.g. run_20251110_1820).",
    )
    parser.add_argument(
        "--report",
        type=Path,
        default=REPORTS_DIR / "single_point_stretch_metrics.json",
        help="Path to save JSON report (default: logs/reports/...).",
    )
    return parser.parse_args()


def discover_run_dirs(base_dir: Path) -> List[Path]:
    runs = []
    if not base_dir.exists():
        return runs
    for candidate in sorted(base_dir.iterdir()):
        if candidate.is_dir() and any(child.name.startswith("stretch_") for child in candidate.iterdir() if child.is_dir()):
            runs.append(candidate)
    return runs


def main():
    args = parse_args()
    data_root = args.data_root
    run_dir: Path

    available_runs = discover_run_dirs(data_root)

    if args.run:
        run_dir = data_root / args.run
        if not run_dir.exists():
            raise RuntimeError(f"Run directory '{run_dir}' does not exist.")
    elif available_runs:
        if len(available_runs) == 1:
            run_dir = available_runs[0]
        else:
            print("Available runs:")
            for idx, run_path in enumerate(available_runs, start=1):
                print(f"  [{idx}] {run_path.name}")
            selection = input("Select a run by number: ").strip()
            try:
                sel_idx = int(selection)
                if sel_idx < 1 or sel_idx > len(available_runs):
                    raise ValueError()
            except ValueError:
                raise RuntimeError("Invalid selection.")
            run_dir = available_runs[sel_idx - 1]
    else:
        run_dir = data_root

    print("=" * 80)
    print(f"Single-point stretch evaluation")
    print(f"Data root: {data_root}")
    print(f"Evaluating run directory: {run_dir}")
    print("=" * 80 + "\n")

    combined_df, grouped = load_dataset(run_dir)

    force_results_full = []
    force_results_subset = []
    offset_results_per_stretch_full = {}
    offset_results_per_stretch_subset = {}
    for stretch_label, df in grouped.items():
        try:
            X_full = np.stack(df["sensor_vector"].to_numpy())
            X_subset = np.stack(df["sensor_vector_subset"].to_numpy())
            fz_vals = df["fz"].to_numpy()
            offsets_vals = df["offset_key"].to_numpy()

            metrics_full, _ = train_force_regressor(X_full, fz_vals, stretch_label)
            metrics_subset, _ = train_force_regressor(X_subset, fz_vals, f"{stretch_label}_subset")
            force_results_full.append(metrics_full)
            force_results_subset.append(metrics_subset)
        except Exception as exc:
            print(f"❌ Error evaluating force mapping for {stretch_label}: {exc}")

        try:
            metrics_offset_full, _ = train_offset_classifier(X_full, offsets_vals, stretch_label)
            metrics_offset_subset, _ = train_offset_classifier(X_subset, offsets_vals, f"{stretch_label}_subset")
            offset_results_per_stretch_full[stretch_label] = metrics_offset_full
            offset_results_per_stretch_subset[stretch_label] = metrics_offset_subset
        except Exception as exc:
            print(f"❌ Error training offset classifier for {stretch_label}: {exc}")

    combined_X_full = np.stack(combined_df["sensor_vector"].to_numpy())
    combined_X_subset = np.stack(combined_df["sensor_vector_subset"].to_numpy())
    combined_offsets = combined_df["offset_key"].to_numpy()
    combined_stretches = combined_df["stretch_label"].to_numpy()
    combined_fz = combined_df["fz"].to_numpy()

    combined_results_full = evaluate_combined_pipeline(
        combined_X_full,
        combined_offsets,
        combined_stretches,
        combined_fz,
    )
    combined_results_subset = evaluate_combined_pipeline(
        combined_X_subset,
        combined_offsets,
        combined_stretches,
        combined_fz,
    )

    # ------------------------------------------------------------------
    # Reporting
    # ------------------------------------------------------------------
    print("\nForce Mapping Metrics (per stretch)")
    print("-" * 80)
    for res in force_results_full:
        print(
            f"{res['stretch_label']}: samples={res['samples']}, "
            f"RMSE={res['rmse']:.4f} N, STD={res['std_dev']:.4f} N, "
            f"ΔF_est={res['force_resolution_est']:.4f} N, "
            f"KPM1={'PASS' if res.get('kpm1_pass') else 'FAIL' if res.get('kpm1_pass') is not None else 'N/A'}, "
            f"KPM2={'PASS' if res['kpm2_pass'] else 'FAIL'}"
        )

    print("\nForce Mapping Metrics (subset sensors)")
    print("-" * 80)
    for res in force_results_subset:
        print(
            f"{res['stretch_label']}: samples={res['samples']}, "
            f"RMSE={res['rmse']:.4f} N, STD={res['std_dev']:.4f} N, "
            f"ΔF_est={res['force_resolution_est']:.4f} N, "
            f"KPM1={'PASS' if res.get('kpm1_pass') else 'FAIL' if res.get('kpm1_pass') is not None else 'N/A'}, "
            f"KPM2={'PASS' if res['kpm2_pass'] else 'FAIL'}"
        )

    print("\nCombined Force Mapping (pooled stretches)")
    print("-" * 80)
    combined_force_metrics_full = combined_results_full["combined_force_metrics"]
    print(
        f"combined (full): samples={combined_force_metrics_full['samples']}, "
        f"RMSE={combined_force_metrics_full['rmse']:.4f} N, "
        f"STD={combined_force_metrics_full['std_dev']:.4f} N, "
        f"ΔF_est={combined_force_metrics_full['force_resolution_est']:.4f} N, "
        f"KPM1={'PASS' if combined_force_metrics_full.get('kpm1_pass') else 'FAIL' if combined_force_metrics_full.get('kpm1_pass') is not None else 'N/A'}, "
        f"KPM2={'PASS' if combined_force_metrics_full['kpm2_pass'] else 'FAIL'}"
    )

    combined_force_metrics_subset = combined_results_subset["combined_force_metrics"]
    print(
        f"combined (subset): samples={combined_force_metrics_subset['samples']}, "
        f"RMSE={combined_force_metrics_subset['rmse']:.4f} N, "
        f"STD={combined_force_metrics_subset['std_dev']:.4f} N, "
        f"ΔF_est={combined_force_metrics_subset['force_resolution_est']:.4f} N, "
        f"KPM1={'PASS' if combined_force_metrics_subset.get('kpm1_pass') else 'FAIL' if combined_force_metrics_subset.get('kpm1_pass') is not None else 'N/A'}, "
        f"KPM2={'PASS' if combined_force_metrics_subset['kpm2_pass'] else 'FAIL'}"
    )

    print("\nOffset Classification Metrics (per stretch, full feature set)")
    print("-" * 80)
    for label, metrics in offset_results_per_stretch_full.items():
        print(f"{label}: samples={metrics['samples']}, accuracy={metrics['accuracy']:.3f}")

    print("\nOffset Classification Metrics (per stretch, subset feature set)")
    print("-" * 80)
    for label, metrics in offset_results_per_stretch_subset.items():
        print(f"{label}: samples={metrics['samples']}, accuracy={metrics['accuracy']:.3f}")

    print("\nCombined Offset Classification (pooled stretches)")
    print("-" * 80)
    combined_offset_metrics_full = combined_results_full["combined_offset_metrics"]
    print(f"Full features → Samples: {combined_offset_metrics_full['samples']}")
    print(f"Accuracy: {combined_offset_metrics_full['accuracy']:.3f}")
    print(combined_offset_metrics_full["report"])

    combined_offset_metrics_subset = combined_results_subset["combined_offset_metrics"]
    print(f"Subset features → Samples: {combined_offset_metrics_subset['samples']}")
    print(f"Accuracy: {combined_offset_metrics_subset['accuracy']:.3f}")
    print(combined_offset_metrics_subset["report"])

    print("\nGated Offset Classification (stretch → offset)")
    print("-" * 80)
    gated_offset_metrics_full = combined_results_full["gated_offset_metrics"]
    print(f"Full features → Samples: {gated_offset_metrics_full['samples']}")
    print(f"Accuracy: {gated_offset_metrics_full['accuracy']:.3f}")
    print(gated_offset_metrics_full["report"])

    gated_offset_metrics_subset = combined_results_subset["gated_offset_metrics"]
    print(f"Subset features → Samples: {gated_offset_metrics_subset['samples']}")
    print(f"Accuracy: {gated_offset_metrics_subset['accuracy']:.3f}")
    print(gated_offset_metrics_subset["report"])

    print("\nStretch Classification Metrics (pooled)")
    print("-" * 80)
    combined_stretch_metrics_full = combined_results_full["combined_stretch_metrics"]
    print(f"Full features → Samples: {combined_stretch_metrics_full['samples']}")
    print(f"Accuracy: {combined_stretch_metrics_full['accuracy']:.3f}")
    print(combined_stretch_metrics_full["report"])

    combined_stretch_metrics_subset = combined_results_subset["combined_stretch_metrics"]
    print(f"Subset features → Samples: {combined_stretch_metrics_subset['samples']}")
    print(f"Accuracy: {combined_stretch_metrics_subset['accuracy']:.3f}")
    print(combined_stretch_metrics_subset["report"])

    # Save JSON report
    report_payload = {
        "force_mapping_per_stretch_full": force_results_full,
        "force_mapping_per_stretch_subset": force_results_subset,
        "force_mapping_combined_full": combined_force_metrics_full,
        "force_mapping_combined_subset": combined_force_metrics_subset,
        "offset_classification_per_stretch_full": offset_results_per_stretch_full,
        "offset_classification_per_stretch_subset": offset_results_per_stretch_subset,
        "offset_classification_combined_full": combined_offset_metrics_full,
        "offset_classification_combined_subset": combined_offset_metrics_subset,
        "offset_classification_gated_full": gated_offset_metrics_full,
        "offset_classification_gated_subset": gated_offset_metrics_subset,
        "stretch_classification_combined_full": combined_stretch_metrics_full,
        "stretch_classification_combined_subset": combined_stretch_metrics_subset,
        "kpm_thresholds": {
            "accuracy_rmse_threshold": ACCURACY_RMSE_THRESHOLD,
            "precision_std_threshold": PRECISION_STD_THRESHOLD,
            "sensitivity_target": SENSITIVITY_TARGET,
        },
    }

    # Determine report path (append run label if default name requested)
    if args.report == REPORTS_DIR / "single_point_stretch_metrics.json":
        report_path = REPORTS_DIR / f"{run_dir.name}_metrics.json"
    else:
        report_path = args.report

    report_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_payload, f, indent=2)

    print(f"\nMetrics written to {report_path}")


if __name__ == "__main__":
    main()

